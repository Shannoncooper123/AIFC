#!/usr/bin/env python3
"""
è¿ç§»è„šæœ¬ï¼šå°†æ—§ç‰ˆå•æ–‡ä»¶ workflow_trace.jsonl æ‹†åˆ†ä¸ºæ–°çš„åˆ†å±‚å­˜å‚¨æ ¼å¼

æ–°æ ¼å¼ï¼š
- workflow_index.jsonl: ç´¢å¼•æ–‡ä»¶ï¼Œå­˜å‚¨æ¯ä¸ª workflow çš„æ‘˜è¦ä¿¡æ¯
- workflow_traces/: ç›®å½•ï¼ŒæŒ‰ workflow_run_id åˆ†æ–‡ä»¶å­˜å‚¨è¯¦ç»† trace

ä½¿ç”¨æ–¹å¼ï¼š
    cd backend
    python scripts/migrate_workflow_traces.py
    
    # æˆ–æŒ‡å®šæºæ–‡ä»¶
    python scripts/migrate_workflow_traces.py --source modules/data/workflow_trace.jsonl
"""
import argparse
import json
import os
import sys
from collections import defaultdict
from typing import Any, Dict, List

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from modules.config.settings import load_config, get_config


def migrate_workflow_traces(source_path: str, dry_run: bool = False) -> None:
    """
    è¿ç§» workflow trace æ•°æ®
    
    Args:
        source_path: æºæ–‡ä»¶è·¯å¾„ï¼ˆæ—§ç‰ˆå•æ–‡ä»¶ï¼‰
        dry_run: æ˜¯å¦åªé¢„è§ˆä¸å®é™…å†™å…¥
    """
    if not os.path.exists(source_path):
        print(f"âŒ æºæ–‡ä»¶ä¸å­˜åœ¨: {source_path}")
        return
    
    cfg = get_config()
    agent_cfg = cfg.get("agent", {})
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    index_path = agent_cfg.get("workflow_index_path", "modules/data/workflow_index.jsonl")
    if not os.path.isabs(index_path):
        index_path = os.path.join(base_dir, index_path)
    
    traces_dir = agent_cfg.get("workflow_traces_dir", "modules/data/workflow_traces")
    if not os.path.isabs(traces_dir):
        traces_dir = os.path.join(base_dir, traces_dir)
    
    print(f"ğŸ“‚ æºæ–‡ä»¶: {source_path}")
    print(f"ğŸ“‚ ç´¢å¼•æ–‡ä»¶: {index_path}")
    print(f"ğŸ“‚ Trace ç›®å½•: {traces_dir}")
    print()
    
    events_by_run: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
    workflow_summaries: Dict[str, Dict[str, Any]] = {}
    total_events = 0
    parse_errors = 0
    
    print("ğŸ“– è¯»å–æºæ–‡ä»¶...")
    with open(source_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                event = json.loads(line)
                total_events += 1
                
                workflow_run_id = event.get("workflow_run_id")
                if not workflow_run_id:
                    continue
                
                events_by_run[workflow_run_id].append(event)
                
                event_type = event.get("type")
                status = event.get("status")
                
                if workflow_run_id not in workflow_summaries:
                    workflow_summaries[workflow_run_id] = {
                        "run_id": workflow_run_id,
                        "start_time": None,
                        "end_time": None,
                        "duration_ms": None,
                        "status": None,
                        "symbols": [],
                        "pending_count": 0,
                        "nodes_count": 0,
                        "tool_calls_count": 0,
                        "model_calls_count": 0,
                        "artifacts_count": 0,
                    }
                
                summary = workflow_summaries[workflow_run_id]
                
                if event_type == "workflow":
                    if status == "running":
                        summary["start_time"] = event.get("start_time")
                        payload = event.get("payload", {})
                        alert = payload.get("alert", {})
                        summary["symbols"] = alert.get("symbols", [])
                        summary["pending_count"] = alert.get("pending_count", 0)
                    else:
                        summary["end_time"] = event.get("end_time")
                        summary["duration_ms"] = event.get("duration_ms")
                        summary["status"] = status
                        if not summary["start_time"]:
                            summary["start_time"] = event.get("start_time")
                elif event_type == "node":
                    summary["nodes_count"] += 1
                elif event_type == "tool_call":
                    summary["tool_calls_count"] += 1
                elif event_type == "model_call":
                    summary["model_calls_count"] += 1
                elif event_type == "artifact":
                    summary["artifacts_count"] += 1
                    
            except json.JSONDecodeError as e:
                parse_errors += 1
                print(f"  âš ï¸ ç¬¬ {line_num} è¡Œè§£æå¤±è´¥: {e}")
                continue
    
    print(f"âœ… è¯»å–å®Œæˆ: {total_events} æ¡äº‹ä»¶, {len(events_by_run)} ä¸ª workflow")
    if parse_errors > 0:
        print(f"  âš ï¸ {parse_errors} æ¡è§£æå¤±è´¥")
    print()
    
    if dry_run:
        print("ğŸ” é¢„è§ˆæ¨¡å¼ (--dry-run)ï¼Œä¸å®é™…å†™å…¥æ–‡ä»¶")
        print()
        print("å°†åˆ›å»ºä»¥ä¸‹æ–‡ä»¶:")
        print(f"  - {index_path} ({len(workflow_summaries)} æ¡è®°å½•)")
        for run_id in sorted(events_by_run.keys())[:10]:
            trace_path = os.path.join(traces_dir, f"{run_id}.jsonl")
            print(f"  - {trace_path} ({len(events_by_run[run_id])} æ¡äº‹ä»¶)")
        if len(events_by_run) > 10:
            print(f"  ... è¿˜æœ‰ {len(events_by_run) - 10} ä¸ªæ–‡ä»¶")
        return
    
    print("ğŸ“ åˆ›å»ºç›®å½•...")
    os.makedirs(os.path.dirname(index_path), exist_ok=True)
    os.makedirs(traces_dir, exist_ok=True)
    
    print("ğŸ“ å†™å…¥ç´¢å¼•æ–‡ä»¶...")
    sorted_summaries = sorted(
        workflow_summaries.values(),
        key=lambda s: s.get("start_time") or "",
        reverse=True
    )
    with open(index_path, "w", encoding="utf-8") as f:
        for summary in sorted_summaries:
            f.write(json.dumps(summary, ensure_ascii=False) + "\n")
    print(f"  âœ… å†™å…¥ {len(sorted_summaries)} æ¡ç´¢å¼•è®°å½•")
    
    print("ğŸ“ å†™å…¥ trace æ–‡ä»¶...")
    for run_id, events in events_by_run.items():
        trace_path = os.path.join(traces_dir, f"{run_id}.jsonl")
        events_sorted = sorted(events, key=lambda e: e.get("timestamp_ms", 0))
        with open(trace_path, "w", encoding="utf-8") as f:
            for event in events_sorted:
                f.write(json.dumps(event, ensure_ascii=False) + "\n")
    print(f"  âœ… å†™å…¥ {len(events_by_run)} ä¸ª trace æ–‡ä»¶")
    
    print()
    print("ğŸ‰ è¿ç§»å®Œæˆ!")
    print()
    print("åç»­æ­¥éª¤:")
    print(f"  1. éªŒè¯æ–°æ–‡ä»¶æ˜¯å¦æ­£ç¡®ç”Ÿæˆ")
    print(f"  2. æµ‹è¯• API æ˜¯å¦æ­£å¸¸å·¥ä½œ")
    print(f"  3. å¯é€‰ï¼šå¤‡ä»½å¹¶åˆ é™¤æ—§æ–‡ä»¶ {source_path}")


def main():
    parser = argparse.ArgumentParser(
        description="è¿ç§» workflow trace æ•°æ®åˆ°æ–°çš„åˆ†å±‚å­˜å‚¨æ ¼å¼"
    )
    parser.add_argument(
        "--source",
        type=str,
        default=None,
        help="æºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä» config.yaml è¯»å– workflow_trace_pathï¼‰"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…å†™å…¥æ–‡ä»¶"
    )
    
    args = parser.parse_args()
    
    load_config()
    cfg = get_config()
    
    if args.source:
        source_path = args.source
    else:
        agent_cfg = cfg.get("agent", {})
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        source_path = agent_cfg.get("workflow_trace_path", "modules/data/workflow_trace.jsonl")
        if not os.path.isabs(source_path):
            source_path = os.path.join(base_dir, source_path)
    
    migrate_workflow_traces(source_path, dry_run=args.dry_run)


if __name__ == "__main__":
    main()
